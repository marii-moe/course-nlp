{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: Training an IMDb sentiment model with *ULMFiT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a quick end-to-end example of training a model. We'll train a sentiment classifier on a sample of the popular IMDb data, showing 4 steps:\n",
    "\n",
    "1. Reading and viewing the IMDb data\n",
    "1. Getting your data ready for modeling\n",
    "1. Fine-tuning a language model\n",
    "1. Building a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to images in Computer Vision, text can't directly be transformed into numbers to be fed into a model. The first thing we need to do is to preprocess our data so that we change the raw texts to lists of words, or tokens (a step that is called tokenization) then transform these tokens into numbers (a step that is called numericalization). These numbers are then passed to embedding layers that will convert them in arrays of floats before passing them through a model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Get your data preprocessed and ready to use,\n",
    "1. Create a language model with pretrained weights that you can fine-tune to your dataset,\n",
    "1. Create other models such as classifiers on top of the encoder of the language model.\n",
    "\n",
    "To show examples, we have provided a small sample of the [IMDB dataset](https://www.imdb.com/interfaces/) which contains 1,000 reviews of movies with labels (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  is_valid\n",
       "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1  positive  This is a extremely well-made film. The acting...     False\n",
       "2  negative  Every once in a long while a movie will come a...     False\n",
       "3  positive  Name just says it all. I watched this movie wi...     False\n",
       "4  negative  This movie succeeds at being one of the most u...     False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data and create a `databunch` object to use for a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure count is 0\n",
      "\n",
      "CPU times: user 242 ms, sys: 263 ms, total: 505 ms\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# throws `BrokenProcessPool` Error sometimes. Keep trying `till it works!\n",
    "count = 0\n",
    "error = True\n",
    "while error:\n",
    "    try: \n",
    "        # The following line throws `AttributeError: backwards` on the learning step, below\n",
    "        # data_lm = TextDataBunch.from_csv(path, 'texts.csv')\n",
    "        # This Fastai Forum post shows the solution:\n",
    "        #      https://forums.fast.ai/t/backwards-attributes-not-found-in-nlp-text-learner/51340?u=jcatanza\n",
    "        # We implement the solution on the following line:\n",
    "        data_lm = TextLMDataBunch.from_csv(path, 'texts.csv')\n",
    "        error = False\n",
    "        print(f'failure count is {count}\\n')    \n",
    "    except: # catch *all* exceptions\n",
    "        # accumulate failure count\n",
    "        count = count + 1\n",
    "        print(f'failure count is {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data again, this time form a `databunch` object for use in a classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure count is 0\n",
      "\n",
      "CPU times: user 305 ms, sys: 264 ms, total: 569 ms\n",
      "Wall time: 1.82 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# throws `BrokenProcessPool` Error sometimes. Keep trying `till it works!\n",
    "count = 0\n",
    "error = True\n",
    "while error:\n",
    "    try: \n",
    "        # Create the databunch for the classifier model\n",
    "        data_clas = TextClasDataBunch.from_csv(path, 'texts.csv', vocab=data_lm.train_ds.vocab, bs=32)\n",
    "        error = False\n",
    "        print(f'failure count is {count}\\n')    \n",
    "    except: # catch *all* exceptions\n",
    "        # accumulate failure count\n",
    "        count = count + 1\n",
    "        print(f'failure count is {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the `databunch` objects for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save('data_lm_export.pkl')\n",
    "data_clas.save('data_clas_export.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the `databunch` objects\n",
    "Note that you can load the data with different [`DataBunch`](/basic_data.html#DataBunch) parameters (batch size, `bptt`,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=192\n",
    "#bs=48\n",
    "#bs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "data_lm = load_data(path, 'data_lm_export.pkl', bs=bs)\n",
    "data_clas = load_data(path, 'data_clas_export.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build and fine-tune a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `data_lm` object we created earlier to fine-tune a pretrained language model. [fast.ai](http://www.fast.ai/) has an English model with an AWD-LSTM architecture available that we can download. We can create a learner object that will directly create a model, download the pretrained weights and be ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(1)\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the IMDb language model, initializing with the hpretrained weights from `wikitext-103`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training\n",
    "By default this step unfreezes and trains the weights for the last layer. Training updates the weights to values more applicable to the language of IMDb reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.375977</td>\n",
       "      <td>4.043857</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_pretrained??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_weights??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use [Visual Studio Code](https://code.visualstudio.com/) (vscode - open source editor that comes with recent versions of Anaconda, or can be installed separately), or most editors and IDEs, to browse code. vscode things to know:\n",
    "\n",
    "- Command palette (<kbd>Ctrl-shift-p</kbd>)\n",
    "- Go to symbol (<kbd>Ctrl-t</kbd>)\n",
    "- Find references (<kbd>Shift-F12</kbd>)\n",
    "- Go to definition (<kbd>F12</kbd>)\n",
    "- Go back (<kbd>alt-left</kbd>)\n",
    "- View documentation\n",
    "- Hide sidebar (<kbd>Ctrl-b</kbd>)\n",
    "- Zen mode (<kbd>Ctrl-k,z</kbd>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze the weights and train some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a computer vision model, we can then unfreeze the model and fine-tune it.\n",
    "In this step we are allowing the model to update *all* the weights with values more suitable to the language of IMDb reviews. But we are mindful that the pretrained weights from wikitext-103 are likely already near their optimal values. For this we train the weights in the \"earlier\" layers with a much lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.076272</td>\n",
       "      <td>3.932792</td>\n",
       "      <td>0.279836</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.870787</td>\n",
       "      <td>3.885299</td>\n",
       "      <td>0.282723</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.676853</td>\n",
       "      <td>3.883216</td>\n",
       "      <td>0.282664</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, slice(1e-4,1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate your language model, you can run the [`Learner.predict`](/basic_train.html#Learner.predict) method and specify the number of words you want it to guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a review about an exercise in screenplay direction is that of the director'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"This is a review about\", n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the hierarchy of horror movies this has to be near the top. But apparently , in terms of his outlandish social'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"In the hierarchy of horror movies this has to be near the top.\", n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the generated text doesn't make much sense because we have a tiny vocabulary and didn't train much. But note that the model respects basic grammar, which comes from the pretrained model.\n",
    "\n",
    "Finally we save the encoder to be able to use it for classification in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('mini_imdb_language_model')\n",
    "learn.save_encoder('mini_imdb_language_model_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a movie review classifier\n",
    "We use mixed precision (`.to_fp16()`)for greater speed, smaller memory footprint, and a regularizing effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (799 items)\n",
       "x: TextList\n",
       "xxbos \" a xxmaj cry in the xxmaj dark \" is a xxunk piece of cinema , haunting , and incredibly though provoking . xxmaj the true story of xxmaj lindy xxmaj xxunk , who , in 1980 , witnessed a horrific sight , seeing her xxunk - old baby being brutally taken from their family 's tent , while camping on the xxmaj xxunk xxunk . xxmaj xxunk ( the baby ) was never seen again , and the result of her horrendous disappearance caused a true life xxunk all around the world . xxmaj meryl xxmaj streep does immaculate justice to the role of xxmaj lindy , as she always does . xxmaj but the one thing that helps \" a xxmaj cry in the xxmaj dark \" never fall flat is the brilliant direction . a truly inspired and accurate outlook on this xxunk case , tears are brought to the eyes . xxmaj the concept is nothing less then terrifying , and afterwards you are left haunted , but also inspired .,xxbos i love this movie . i mean the story may not be the best , but the dancing most certainly makes up for it . xxmaj you get to know a little bit about each character the way xxup they want you to learn about them . i just think that you wo n't like this movie unless you 're into xxmaj broadway ...,xxbos xxmaj this was another obscure xxmaj christmas - related title , a low - budget xxmaj mexican production from exploitation film - maker xxmaj cardona ( xxup night xxup of xxup the xxup bloody xxup apes [ 1969 ] , xxup xxunk ! [ 1977 ] ) , which  like many a genre effort from this country  was acquired for release in the xxup u.s. by xxup xxunk xxmaj gordon xxmaj murray . xxmaj judging by those two efforts already mentioned , xxmaj cardona was no xxunk  and , this one having already received its share of xxunk over here , is certainly no better ! xxmaj the film , in fact , is quite xxunk of the xxunk which xxunk xxmaj mexican horror xxunk from the era , but given an added dimension by virtue of the garish color ( which , in view of the xxunk of reds  apart from xxmaj st. xxmaj nick himself , the xxmaj devil plays a major role in the proceedings  throughout , was essential ) . xxmaj anyway , in a nutshell , the plot involves xxmaj satan 's efforts to xxunk xxmaj santa xxmaj claus ' xxmaj christmas xxmaj eve xxunk with the xxmaj earth 's children ; there is , however , plenty more wackiness along the way : to begin with , our xxunk , white - xxunk and xxunk merry man - in - red lives in a celestial palace who , apart from accompanying toy - maker kids from all over the world on his piano as they sing ( xxunk for the whole first reel ! ) in their native tongue , visits xxmaj merlin  the famed magician at xxmaj king xxmaj arthur 's court , here xxunk but amusingly prone to child - like hopping and xxunk xxunk !  once every year to acquire xxunk which would bring xxunk to the young and render himself invisible ( by the way , the xxmaj wizard 's anachronistic presence here is no less unlikely than his being a xxunk of xxmaj dr. xxmaj xxunk in xxup son xxup of xxmaj dracula [ 1974 ] ! ! ) ; incidentally , by this time , he always seems to have gained some excess weight \n",
       " so xxmaj santa has to work out in order to be able to fit into each xxunk xxunk ! xxmaj the xxmaj devil 's antics ( xxunk rubbing his hands together at every turn and generally xxunk it up ) to hold up xxmaj st. xxmaj nick 's delivery program , then , is perfectly puerile : indeed , their tit - for - tat shenanigans resemble an old xxmaj laurel & xxmaj hardy routine more than anything ! xxmaj to pad out the running - time , we focus on three sets of children : one , the lonely son of a rich couple who wants nothing more for xxmaj christmas than their company ( projected as a wish - xxunk fantasy where the boy finds his parents wrapped in extra - large xxunk ! ) , a girl from a poor family who xxunk to own a doll of her own ( the horned one first xxunk her to steal one , then xxunk the little one 's dreams  to no xxunk ) and a trio of xxunk who , xxunk on once again by xxmaj satan , think of nothing but causing mischief and eventually fall out amongst themselves . xxmaj there is definitely imagination at work here , but it is xxunk with little xxunk or reason , while the overall juvenile approach keeps entertainment ( unless one counts the film as a guilty pleasure ) well at bay !,xxbos xxmaj it 's a very good movie , not only for the fans of xxmaj lady xxmaj death comics , but also for those who like animated movies / series of adventure and fantasy . \n",
       " \n",
       "  xxmaj the film is about a innocent girl who is about killed for something she had n't done , but for be who she is daughter of the xxunk of hell , xxmaj xxunk himself . \n",
       " \n",
       "  xxmaj then she seeks revenge ... and the rest you better see it from the movie . \n",
       " \n",
       "  i liked the movie a lot , the characters are like the original comics , form xxmaj chaos . i never had the chance of read the the first parts of the story in comics , only the last ones , after the xxunk in the movie , so i can not tell you if the events are exactly like the comics , but ... one way or another it 's the story of xxmaj lady xxmaj death !,xxbos i am terribly sorry , i know that xxmaj xxunk still is called one of the greatest directors in post - war xxmaj germany and that most of his films are considered \" master - pieces \" , but when i see \" xxmaj lili xxmaj marleen \" today , in 2004 , i wonder what everyone is up and away about this movie ! xxmaj the acting is simply terrible - xxmaj xxunk xxmaj xxunk is all the smiling like an idiot ! - , the xxunk between xxmaj nazi - glamour and xxunk are ridiculous , the whole film looks as if it was made within two days in an attic . xxmaj probably it was exactly that way and many people seem to take this for \" real art \" , but for me this movie is simply bad & cheap . xxmaj compare this to xxmaj xxunk \" xxmaj la xxmaj xxunk xxunk xxmaj xxunk \" and tell me again that \" xxmaj lili xxmaj marleen \" is a good movie ...\n",
       "y: CategoryList\n",
       "positive,positive,negative,positive,negative\n",
       "Path: /home/molly/.fastai/data/imdb_sample;\n",
       "\n",
       "Valid: LabelList (201 items)\n",
       "x: TextList\n",
       "xxbos xxmaj though i liked xxmaj on the xxmaj town better i really liked it . i 'm a new xxunk when it comes to xxmaj frank xxmaj sinatra and xxmaj gene xxmaj kelly . xxmaj though i had heard of them i had never seen anything with them in it until recently . xxmaj the first one i saw was xxmaj xxunk in the xxmaj rain that made me a fan of xxmaj gene 's . i think that is better too . xxmaj but i thought that this movie was good and like all movies there are some parts that are better than others but in my book it 's an awesome movie and i love it . xxmaj frank and xxmaj gene make a good team . i have yet to see them together in xxmaj take me out to the xxmaj xxunk . xxmaj but i 'm sticking to my guns xxunk saying that i really enjoyed it , and that i love it !,xxbos xxmaj this is the best film the xxmaj derek couple has ever made and if you think this is a recommendation then you have n't seen any of the others . xxmaj there are the usual ingredients : it is just as poorly acted as their other efforts , we can watch xxmaj bo disrobing or xxunk for wet t - shirt contests quite frequently , the story is just laughably idiotic , and the film takes itself much too seriously . xxmaj and then : xxmaj xxunk xxmaj xxunk in xxmaj africa ? \n",
       " \n",
       "  xxmaj but it has a few things going for it . xxmaj bo looks great , the production values ( sets , costumes , etc . ) are quite good , and this greatly enhances its camp value . xxmaj in a strange way it is actually quite funny , simply because it tries to be serious and fails so badly .,xxbos xxmaj like most people i was intrigued when i heard the concept of this film , especially the \" film makers were then attacked \" aspect that the case seems to emphasize , what with the picture on the cover of the film makers being chased by an angry mob . \n",
       " \n",
       "  xxmaj then , to watch the film and discover , oh , what they mean by \" the film makers were attacked \" was some kids threw rocks at a sign and a number of people xxunk loudly and said \" xxmaj someone should beat those two kids up . \" xxmaj the picture on the cover , \" the chase \" as it were ? xxmaj total fabrication . xxmaj which i guess ties in with the theme of the film , lying and manipulation to satisfy vain , stupid children with more money and time then sense . \n",
       " \n",
       "  i have no idea what great truth the viewer is supposed to take away from this film . xxmaj it 's like xxmaj michael xxmaj moore 's \" xxmaj roger & xxmaj me \" , but if \" xxmaj roger & xxmaj me \" was xxmaj moore mocking the people of xxmaj xxunk . xxmaj it 's completely xxunk and totally inane . xxmaj wow ! xxmaj can you believe that people who suffered under the xxunk of xxmaj communism would be really excited to have xxunk full of food ? xxmaj what jerks ! xxmaj and it 's not so much , \" xxmaj look at the effects of capitalism and western media blah blah blah \" , since it was n't just that their fake market had comparable prices to the competitors , it was that , as many people in the film say , the prices were absurdly low , someone mentions that they should 've known it was fake by how much they were xxunk for duck . xxmaj that 's not proving anything except that people who are poor , will go to a store that has low prices , bravo xxunk , way to stick it to the people on the bottom . \n",
       " \n",
       "  xxmaj way to play a stupid practical joke on elderly people . xxmaj you should be very proud . xxmaj how about for your next movie you make a documentary about xxmaj iraq and show how people there will get really excited for a house without bullet holes in the walls and then , say , \" xxup haha ! xxup no xxup such xxup house xxup exists ! xxup your xxup so xxup stupid xxup and xxup loved xxup to xxup be xxup lied xxup to xxup by xxup the xxup media ! \" . \n",
       " \n",
       "  xxmaj morgan \" xxmaj please xxmaj like xxmaj me \" xxmaj spurlock xxunk this wet fart of a film and it 's no surprise since xxmaj spurlock as xxmaj one xxmaj hit xxmaj wonder prince of the documentary world seems to throw his weight behind any silly sounding concept to stay relevant in a world that really has no need of him . \n",
       " \n",
       "  xxmaj avoid like the plague .,xxbos xxmaj vaguely reminiscent of great 1940 's westerns , like \" xxmaj the xxmaj treasure xxmaj of xxmaj the xxmaj xxunk xxmaj madre \" ( 1948 ) , \" xxmaj red xxmaj rock xxmaj west \" is a story about conscience , greed , and betrayal . xxmaj michael ( xxmaj nicolas xxmaj cage ) is a down and out , but honest , young man from xxmaj texas who goes west in search of work and money . xxmaj he finds both , but not in the way he had expected . \n",
       " \n",
       "  xxmaj the film 's screenplay contains plenty of surprises and plot twists . xxmaj excellent cinematography , xxunk film editing , and moody western music add tension and suspense . xxmaj the xxunk of the big sky country provides a wonderful setting . xxmaj and the acting ranges from good to excellent , with great performances from xxmaj dennis xxmaj hopper and xxup j.t. xxmaj walsh . xxmaj dwight xxmaj yoakam 's specially recorded country / western song provides the film with a strong finale . \n",
       " \n",
       "  xxmaj correctly labeled as neo - noir , \" xxmaj red xxmaj rock xxmaj west \" strikes me as being something else , as well . xxmaj the plot is full of amazing coincidences and improbable timing , so much so that others may regard the screenplay as flawed . xxmaj ordinarily , i would agree . xxmaj in this case , however , when combined with the moody atmosphere , and the fact that the small town of xxmaj red xxmaj rock seems almost empty of normal daily life , the coincidences and unlikely timing suggest a story that , beyond \" xxunk \" , is ... surreal . xxmaj it 's almost as if fate deliberately intervenes with improbable events so as to force xxmaj michael to come to grips with himself . xxmaj from this point of view , the coincidences are not script flaws at all . xxmaj they are necessary plot points in a nightmarish story of a young man who must confront his own demons ... xxunk as other characters . \n",
       " \n",
       "  xxmaj all we need here is xxmaj xxunk xxmaj xxunk , in a postscript , explaining , in his always clearly xxunk voice , that ... a young man , searching for himself , stops in a small , almost deserted town a thousand miles from nowhere . xxmaj it 's his final xxunk in a journey to ... the twilight zone .,xxbos xxmaj when xxmaj marlene xxmaj dietrich was labeled box office poison in 1938 one of a handful of actresses so named by the xxunk xxunk , it was films like xxmaj the xxmaj garden xxmaj of xxmaj allah . xxmaj how a film could be so breathtakingly beautiful to behold and be so xxunk dull is beyond me . xxmaj also how xxmaj marlene if she was trying to xxunk her range and not play a sexpot got stuck with such an old fashioned story is beyond me . \n",
       " \n",
       "  xxmaj the xxmaj garden xxmaj of xxmaj allah , one of the very first films in modern technicolor was a novel set at the turn of the last century by xxmaj robert xxmaj xxunk who then xxunk on a play adaption with xxmaj mary xxmaj anderson that ran for xxunk performances in xxunk - 12 . xxmaj it then got two silent screen xxunk . xxmaj the story is about a monk who runs away from the monastery out in xxmaj french xxmaj xxunk to see some of what he 's missed in the world . xxmaj he runs into a similarly xxunk woman who was xxunk and spent her prime years caring for a sick parent . xxmaj she 's traveling now in the desert and the two meet on a train . \n",
       " \n",
       "  xxmaj the woman is of course xxmaj marlene and the runaway monk is xxmaj charles xxmaj boyer . i 'm not sure what was in xxmaj david xxup xxunk xxmaj xxunk 's mind in filming this story . xxmaj someone like xxmaj ingrid xxmaj bergman might have made it palatable for the audience . xxmaj but you can bet that the movie - going public of 1936 when they xxunk their money down for a ticket they expected to see xxmaj marlene as a modern day xxmaj xxunk rather than a saint with that title . xxmaj the public still remembered xxmaj xxunk xxmaj xxunk and you can bet that it was some desert romance and seduction that they were expecting . \n",
       " \n",
       "  xxmaj as for the monks you have to remember that they are self supporting in their xxunk and this particular one xxunk a special wine of which xxmaj boyer happens to be the one with the secret . xxmaj the monastery will have to xxunk it 's economics if xxmaj boyer leaves . xxmaj the monks are a sincerely xxunk group , but from the head man xxmaj charles xxmaj xxunk on down they 've a right to be a little concerned with some self interest . \n",
       " \n",
       "  xxmaj anyway a whole lot of religious xxunk get said here by a pair of leads that really are not suited for the parts . xxmaj most especially xxmaj marlene xxmaj dietrich . i would watch this film with an eye for the special color desert cinematography and forget the plot .\n",
       "y: CategoryList\n",
       "positive,negative,negative,positive,negative\n",
       "Path: /home/molly/.fastai/data/imdb_sample;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(9056, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(9056, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f697856ddc0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/molly/.fastai/data/imdb_sample'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0, MixedPrecision\n",
       "learn: ...\n",
       "loss_scale: 65536\n",
       "max_noskip: 1000\n",
       "dynamic: True\n",
       "clip: None\n",
       "flat_master: False\n",
       "max_scale: 16777216\n",
       "loss_fp32: True], layer_groups=[Sequential(\n",
       "  (0): Embedding(9056, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(9056, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5).to_fp16()\n",
    "learn.load_encoder('mini_imdb_language_model_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  idx_min = (t != self.pad_idx).nonzero().min()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n \\n  xxmaj it 's usually satisfying to watch a film director change his style /</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos i really wanted to love this show . i truly , honestly did . \\n \\n  xxmaj for the first time , gay viewers get their own version of the \" xxmaj the xxmaj xxunk \" . xxmaj with the help of his obligatory \" hag \" xxmaj xxunk , xxmaj james , a good looking , well - to - do thirty - something has the chance</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos \\n \\n  i 'm sure things did n't exactly go the same way in the real life of xxmaj homer xxmaj hickam as they did in the film adaptation of his book , xxmaj rocket xxmaj boys , but the movie \" xxmaj october xxmaj sky \" ( an xxunk of the book 's title ) is good enough to stand alone . i have not read xxmaj</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj to review this movie , i without any doubt would have to quote that memorable scene in xxmaj tarantino 's \" xxmaj pulp xxmaj fiction \" ( xxunk ) when xxmaj jules and xxmaj vincent are talking about xxmaj mia xxmaj wallace and what she does for a living . xxmaj jules tells xxmaj vincent that the \" xxmaj only thing she did worthwhile was pilot \" .</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj how viewers react to this new \" adaption \" of xxmaj shirley xxmaj jackson 's book , which was promoted as xxup not being a remake of the original 1963 movie ( true enough ) , will be based , i suspect , on the following : those who were big fans of either the book or original movie are not going to think much of this one</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the last layer of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.701952</td>\n",
       "      <td>0.743350</td>\n",
       "      <td>0.512438</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze the classifier model and fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.512905</td>\n",
       "      <td>0.627407</td>\n",
       "      <td>0.691542</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478756</td>\n",
       "      <td>0.559037</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.417197</td>\n",
       "      <td>0.532593</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, slice(1e-4, 1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the classifier\n",
    "We can use our model to predict on a few example of movie review-like raw text by using the [`Learner.predict`](/basic_train.html#Learner.predict) method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is 70% sure that this is a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category tensor(0), tensor(0), tensor([0.7929, 0.2071]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('Although there was lots of blood and violence, I did not think this film was scary enough.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is 83% sure that this is a positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category tensor(0), tensor(0), tensor([0.6681, 0.3319]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('Not so good World War II epic film')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom line: the model did not do a good job, misclassifying both reviews (with high confidence, to boot!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train some more, and re-try these examples to see if we can do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.301811</td>\n",
       "      <td>0.475695</td>\n",
       "      <td>0.815920</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.276673</td>\n",
       "      <td>0.452019</td>\n",
       "      <td>0.796020</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246722</td>\n",
       "      <td>0.388263</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n",
      "/home/molly/miniconda3/envs/fastai1-8/lib/python3.8/site-packages/torch/nn/modules/module.py:1332: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if p.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, slice(1e-4, 1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-try the above examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category tensor(1), tensor(1), tensor([0.3765, 0.6235]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('Although there was lots of blood and violence, I did not think this film was scary enough.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category tensor(1), tensor(1), tensor([0.1834, 0.8166]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('Not so good World War II epic film')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to have benefitted from the extra training, since it now correctly classifies both reviews as `negative`, with high confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make up your own text in the style of a movie review. Then use our classifier to see what it thinks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jekyll": {
   "keywords": "fastai",
   "summary": "Application to NLP, including ULMFiT fine-tuning",
   "title": "text"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
